{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Remember that a <a href=\"probability.html\">Random Variable</a> is a mapping $ X: \\Omega \\rightarrow \\mathbb{R}$ that assigns a real number $X(\\omega)$ to each outcome $\\omega$ in a sample space $\\Omega$.  The definitions below are taken from Larry Wasserman’s All of Statistics.</p>\n",
    "\n",
    "<h2 id=\"cumulative-distribution-function\">Cumulative distribution Function</h2>\n",
    "\n",
    "<p>The <strong>cumulative distribution function</strong>, or the <strong>CDF</strong>, is a function</p>\n",
    "\n",
    "<p>$$F_X : \\mathbb{R} → [0, 1]$$,</p>\n",
    "\n",
    "<p>defined by</p>\n",
    "\n",
    "$$F_X (x) = p(X \\le x).$$\n",
    "\n",
    "<p>A note on notation: $X$ is a random variable while $x$ is a particular value of the random variable.</p>\n",
    "\n",
    "<p>Let $X$ be the random variable representing the number of heads in two coin tosses. Then $x$ can take on values 0, 1 and 2. The CDF for this random variable can be drawn thus (taken from All of Stats):</p>\n",
    "\n",
    "<p><img src=\"../images/2tosscdf.png\" alt=\"\" /></p>\n",
    "\n",
    "<p>Notice that this function is right-continuous and defined for all $x$, even if $x $does not take real values in-between the integers.</p>\n",
    "\n",
    "<h2 id=\"probability-mass-and-distribution-function\">Probability Mass and Distribution Function</h2>\n",
    "\n",
    "<p>$X$ is called a <strong>discrete random variable</strong> if it takes countably many values ${x_1, x_2,…}$. We define the <strong>probability function</strong> or the <strong>probability mass function</strong> (<strong>pmf</strong>) for X by:</p>\n",
    "\n",
    "$$f_X(x) = p(X=x)$$\n",
    "\n",
    "<p>$f_X$ <strong>is a probability</strong>.</p>\n",
    "\n",
    "<p>The pmf for the number of heads in two coin tosses (taken from All of Stats) looks like this:</p>\n",
    "\n",
    "<p><img src=\"../images/2tosspmf.png\" alt=\"\" /></p>\n",
    "\n",
    "<p>On the other hand, a random variable is called a <strong>continuous random variable</strong> if there exists a function $f_X$ such that $f_X (x) \\ge 0$ for all x,  $\\int_{-\\infty}^{\\infty} f_X (x) dx = 1$ and for every a ≤ b,</p>\n",
    "\n",
    "$$\n",
    "p(a < X < b) = \\int_{a}^{b} f_X (x) dx $$\n",
    "\n",
    "<p>The function $f_X$ is called the probability density function (pdf). We have the CDF:</p>\n",
    "\n",
    "$$F_X (x) = \\int_{-\\infty}^{x}f_X (t) dt$$\n",
    "\n",
    "<p>and $f_X (x) = \\frac{d F_X (x)}{dx}$ at all points x at which $F_X$ is differentiable.</p>\n",
    "\n",
    "<p>Continuous variables are confusing. Note:</p>\n",
    "\n",
    "<ol>\n",
    "  <li>$p(X=x) = 0$ for every $x$. You <strong>cant think</strong> of $f_X(x)$ as $p(X=x)$. This holds only for discretes. You can only get probabilities from a pdf by integrating, if only over a very small paty of the space.</li>\n",
    "  <li>A pdf can be bigger than 1 unlike a probability mass function, since probability masses represent actual probabilities.</li>\n",
    "</ol>\n",
    "\n",
    "<h3 id=\"a-continuous-example-the-uniform-distribution\">A continuous example: the Uniform Distribution</h3>\n",
    "\n",
    "<p>Suppose that X has pdf\n",
    "$$\n",
    "f_X (x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{for } 0 \\leq x\\leq 1\\\\\n",
    "    0             & \\text{otherwise.}\n",
    "\\end{cases} $$\n",
    "A random variable with this density is said to have a Uniform (0,1) distribution. This is meant to capture the idea of choosing a point at random between 0 and 1. The cdf is given by:\n",
    "\n",
    "$$\n",
    "F_X (x) =\n",
    "\\begin{cases}\n",
    "0 & x \\le 0\\\\\n",
    "x & 0 \\leq x \\leq 1\\\\\n",
    "1 & x > 1.\n",
    "\\end{cases} $$\n",
    "and can be visualized as so (again from All of Stats):</p>\n",
    "\n",
    "<p><img src=\"../images/unicdf.png\" alt=\"\" /></p>\n",
    "\n",
    "<h3 id=\"a-discrete-example-the-bernoulli-distribution\">A discrete example: the Bernoulli Distribution</h3>\n",
    "\n",
    "<p>The <strong>Bernoulli Distribution</strong> represents the distribution a coin flip. Let the random variable $X$ represent such a coin flip, where $X=1$ is heads, and $X=0$ is tails. Let us further say that the probability of heads is $p$ ($p=0.5$ is a fair coin).</p>\n",
    "\n",
    "<p>We then say:</p>\n",
    "\n",
    "$$X \\sim Bernoulli(p)$$\n",
    "\n",
    "<p>which is to be read as $X$ <strong>has distribution</strong> $Bernoulli(p)$. The pmf or probability function associated with the Bernoulli distribution is\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "1 - p & x = 0\\\\\n",
    "p & x = 1.\n",
    "\\end{cases}$$\n",
    "\n",
    "<p>for p in the range 0 to 1. This pmf may  be written as</p>\n",
    "\n",
    "$$f(x) = p^x (1-p)^{1-x}$$\n",
    "\n",
    "<p>for x in the set {0,1}.</p>\n",
    "\n",
    "<p>$p$ is called a parameter of the Bernoulli distribution.</p>\n",
    "\n",
    "<h2 id=\"conditional-and-marginal-distributions\">Conditional and Marginal Distributions</h2>\n",
    "\n",
    "<p>Marginal mass functions are defined in analog to <a href=\"probability.html\">probabilities</a>. Thus:</p>\n",
    "\n",
    "$$f_X(x) = p(X=x) =  \\sum_y f(x, y);\\,\\, f_Y(y) = p(Y=y) = \\sum_x f(x,y)$$\n",
    "\n",
    "<p>Similarly, marginal densities are defined using integrals:</p>\n",
    "\n",
    "$$f_X(x) = \\int dy f(x,y);\\,\\, f_Y(y) = \\int dx f(x,y)$$\n",
    "\n",
    "<p>Notice there is no interpretation of the marginal densities in the continuous case as probabilities. An example here if $f(x,y) = e^{-(x+y)}$ defined on the positive quadrant. The marginal is an exponential defined on the positive part of the line.</p>\n",
    "\n",
    "<p>Conditional mass function is similarly, just a conditional probability. So:</p>\n",
    "\n",
    "$$f_{X \\mid Y}(x \\mid y) = p(X=x \\mid Y=y) = \\frac{p(X=x, Y=y)}{p(Y=y)} = \\frac{f_{XY}(x,y)}{f_Y(y)}$$\n",
    "\n",
    "<p>The similar formula for continuous densities might be suspected to a bit more complex, because we are conditioning on the event $Y=y$ which strictly speaking has 0 probability. But it can be proved that the same formula holds for densities with some additional requirements and interpretation:</p>\n",
    "\n",
    "$$f_{X \\mid Y}(x \\mid y)  = \\frac{f_{XY}(x,y)}{f_Y(y)}$$\n",
    "\n",
    "<p>where we must assume that $f_Y(y) \\gt 0$. Then we have the interpretation that for some event A:</p>\n",
    "\n",
    "$$p(X \\in A \\mid Y=y) = \\int_{x \\in A} f_{X \\mid Y}(x,y) dx$$\n",
    "\n",
    "<p>An example of this is the uniform distribution on the unit square. Suppose then that $y=0.3$. Then the conditional density is a uniform density on the line between 0 and 1 at $y=0.3$.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
