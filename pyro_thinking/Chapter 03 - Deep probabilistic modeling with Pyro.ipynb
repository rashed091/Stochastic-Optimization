{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\">2.1</span> Course thesis</h2>\n",
    "<p>The goal of this course is to learn techniques of causal inference in a way that builds on students’ existing intuition and experience with generative machine learning. Moreover, we will do so using frameworks from generative machine learning, include tools for building deep neural networks. Further, when reasoning about causal inference problems, we will bias the case studies to those seen in professional environments where data scientists and machine learning engineers build and manage in-product machine learning models.</p>\n",
    "<div id=\"causal-modeling-as-generative-ml\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">2.1.1</span> Causal modeling as generative ML</h3>\n",
    "<p>More specifically, this course focuses on machine learning in the following two ways.</p>\n",
    "<ul>\n",
    "<li>We will place causal inference firmly on a foundation of model-based generative machine learning. Our goal is to build machine learning systems that think in causal terms, such as confounding, interventions, and counterfactuals.</li>\n",
    "<li>If you peruse the causal inference literature, you will see examples similar to the A/B test example from epidemiology, econometrics, and clinical trials. This course focuses on the kinds of cases data scientists experience in professional settings, particularly in the tech industry. The focus of the tech industry is shifting towards problems where A/B test becomes more complicated and not feasible. We will cover some advanced techniques like how to deal with confounding, how to build up an online and offline learning and policy evaluation for Markov decision processes that automates testing. We will also cover a little bit of relevant literature from Game theory (Auction models) and Reinforcement learning (policy evaluation and improvement) at the end.</li>\n",
    "</ul>\n",
    "</div>\n",
    "<div id=\"what-is-left-out\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">2.1.2</span> What is left out</h3>\n",
    "<p>Causal inference spans many other concepts, and we won’t be able to cover all of them. Though the concepts below are essential, they are out of scope for this course.</p>\n",
    "<ul>\n",
    "<li>Causal discovery</li>\n",
    "<li>Causal inference with regression models and various canonical SCM models</li>\n",
    "<li>Doubly-robust estimation</li>\n",
    "<li>Interference due to network effects (important in social network tech companies like Facebook or Twitter)</li>\n",
    "<li>heterogeneous treatment effects</li>\n",
    "<li>deep architectures for causal effect inference</li>\n",
    "<li>causal time series models</li>\n",
    "<li>algorithmic information theory approaches to causal inference</li>\n",
    "</ul>\n",
    "</div>\n",
    "<div id=\"examples-of-problems-in-causal-inference\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">2.1.3</span> Examples of problems in causal inference</h3>\n",
    "<p>To properly contextualize our motivation, we start by understanding how causal inference developed as a field across domains, including economics, biology, social science, computer science, anthropology, epidemiology, statistics.</p>\n",
    "<div id=\"estimation-of-causal-effects\" class=\"section level4\">\n",
    "<h4><span class=\"header-section-number\">2.1.3.1</span> Estimation of causal effects</h4>\n",
    "<p>The problem of finding causal effects is the primary motivation of researchers in these domains. For example, in the late 80s and 90s, doctors used to prescribe <a href=\"https://www.mayoclinic.org/diseases-conditions/menopause/in-depth/hormone-replacement-therapy/art-20047550\">Hormonal replacement therapy</a> to old women. Experts believed that at the lower age, women have a lower risk of heart disease than men do, but as they age, after menopause, their estrogen level decline. However, after doing a large randomized trial, where women were selected randomly and given either a placebo or estrogen, the results showed that taking estrogen increases the chance of getting heart disease. Causal inference techniques are essential because the stakes are quite high.</p>\n",
    "</div>\n",
    "<div id=\"counterfactual-reasoning-with-statistics\" class=\"section level4\">\n",
    "<h4><span class=\"header-section-number\">2.1.3.2</span> Counterfactual reasoning with statistics</h4>\n",
    "<p>Counterfactual reasoning means observing reality, and then imagining how reality would have unfolded differently had some causal factor been different. For example, “had I broken up with my girlfriend sooner, I would be much happier today” or “had I studied harder for my SATs, I would be in a much better school today.” An example of a question from an experimental context would be “This subject took the drug, and their condition improved. What is the difference between this amount improvement and the improvement they would have seen had they taken placebo?”</p>\n",
    "<p>Counterfactual reasoning is fundamental to how we as humans reason. However, statistical methods are generally not equipped to enable this type of logic. Your counterfactual reasoning process works with data both from actual and hypothetical realities, while your statistical procedure only has access to data from actual reality.</p>\n",
    "<p>The same is true of cutting-edge machine learning. Intuition tells us that if we trained the most powerful deep learning methods to provide us with relationship advice based on our romantic successes and failers, something would be lacking in that advice since those counterfactual outcomes are missing from the training data.</p>\n",
    "</div>\n",
    "<div id=\"the-challenge-of-running-experiments\" class=\"section level4\">\n",
    "<h4><span class=\"header-section-number\">2.1.3.3</span> The challenge of running experiments</h4>\n",
    "<p>In traditional statistics, randomized experiments are the gold standard for discovering the causal effect. An example of a randomized experiment is an A/B test on a new feature in an app. We randomly assign users to two groups and let one group use the feature while the other is presented with a control comparison. We then observe some key outcome, such as conversions. As we will learn, the randomization enables us to conclude the difference between the two groups is the causal effect of the feature on the conversions, because it isolates that effect from other unknown factors that are also affecting the conversions.</p>\n",
    "<p>However, in many instances, setting up this randomization might be complicated. What if users object to not getting a feature that other users are enjoying? What if the experience of the feature and probability of conversion both depend on user-related factors, such that it is unclear how to do proper randomization? What if some users object to being the subjects of an experiment? What if it is unethical to do the experiment?</p>\n",
    "</div>\n",
    "</div>\n",
    "</div>\n",
    "<div id=\"causal-modeling-as-an-extension-of-generative-modeling\" class=\"section level2\">\n",
    "<h2><span class=\"header-section-number\">2.2</span> Causal modeling as an extension of generative modeling</h2>\n",
    "<div id=\"generative-vs.discriminative-models\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">2.2.1</span> Generative vs. discriminative Models</h3>\n",
    "<p>Let’s focus on supervised learning for a moment. Given a target variable Y and predictor(s) X, a discriminative model learns as much about <span class=\"math inline\">\\(P(Y| X)\\)</span> as it needs to an optimal prediction.</p>\n",
    "<p>In contrast, generative models try to fully learn the joint distribution <span class=\"math inline\">\\(P(X, Y)\\)</span> underlying the data. We will discuss this more in later lectures. In simple words, these models can generate data that looks like real data.</p>\n",
    "<p>We focus on generative models because they allow us to build our theories about the data-generating process into the model itself. We will see that we naturally think of this process in causal terms.</p>\n",
    "</div>\n",
    "<div id=\"model-based-ml-and-learning-to-think-about-the-data-generating-process\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">2.2.2</span> Model-based ML and learning to think about the data-generating process</h3>\n",
    "<p>The following is the typical checklist in training a statistical machine learning model.</p>\n",
    "<ol style=\"list-style-type: decimal\">\n",
    "<li>Split the data into training and test sets.</li>\n",
    "<li>Choose a few models from literally thousands of algorithm choices. Typically this choice is limited algorithms you are familiar with, are in vogue, or happen to be implemented in the software you have available.</li>\n",
    "<li>Manipulate the data until it fits your algorithm inputs and outputs.</li>\n",
    "<li>Evaluate the model on test data, compare to other models</li>\n",
    "<li>( optional) If data doesn’t fit the algorithms modeling assumptions, manipulate the data until it does.</li>\n",
    "<li>(optional) If using a deep learning algorithm, search for hyperparameter settings that further optimize prediction.</li>\n",
    "</ol>\n",
    "<p>This process works well. However, in this workflow, the data scientist’s time is devoted to manipulating data, hyperparameters, and often, the problem definition itself until things work.</p>\n",
    "<p>An alternative approach is to think hard about the process that generated the data, and then explicitly building your assumptions about that process into a bespoke solution tailored to each new problem. This approach is model-based machine learning. Proponents like this approach because with an excellent model-based machine learning framework, you can create a bespoke solution to pretty much any problem, and don’t need to learn a vast number of machine learning algorithms and techniques.</p>\n",
    "<p>Most interestingly, with the model-based machine learning approach the data scientists shifts her time from transforming her problem to fit some standard algorithm, to thinking hard about the process that generated the problem data, and then building those assumptions into the designing of the algorithm.</p>\n",
    "<p>We’ll see in this class that when we think about the data-generating process, we think about it causally, meaning it has some ordering of causes and effects. In this class, we formalize this intuition by apply causal inference theory to model-based machine learning.</p>\n",
    "</div>\n",
    "<div id=\"note-on-reinforcement-learning\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">2.2.3</span> Note on reinforcement learning</h3>\n",
    "<p>As reinforcement learning gains in popularity amongst machine learning researchers and practitioners, many may have encountered the term “model-based” for the first time in a reinforcement learning (RL) context. Model-based RL is indeed an example of model-based machine learning.</p>\n",
    "<ol style=\"list-style-type: decimal\">\n",
    "<li>Model-free RL. The agent has no model of the generating process of the data it perceives in the environment; i.e., how states and actions lead to new states and rewards. It can only learn in a Pavlovian sense, relying solely upon experience.</li>\n",
    "</ol>\n",
    "<ol start=\"2\" style=\"list-style-type: decimal\">\n",
    "<li>Mode-based RL: The agent has a model of the generating process of the data it perceives in the environment. This model enables the agent to make use not only of experience but also of model-based predictions of the consequences of particular actions it has less experience performing.</li>\n",
    "</ol>\n",
    "</div>\n",
    "</div>\n",
    "<div id=\"case-studies\" class=\"section level2\">\n",
    "<h2><span class=\"header-section-number\">2.3</span> Case studies</h2>\n",
    "<div id=\"from-linear-regression-to-model-based-machine-learning\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">2.3.1</span> From linear regression to model-based machine learning</h3>\n",
    "<p>The standard Gaussian linear regression model is represented as follows:</p>\n",
    "<p><span class=\"math display\">\\[\\begin{align} \\epsilon &amp;\\sim \\mathcal{N}(0,1)\\\\ Y &amp;= \\beta X + \\alpha + \\epsilon \\end{align}\\]</span></p>\n",
    "<p>When we read this model specification, it is natural to think of it as predictors <span class=\"math inline\">\\(X\\)</span> generating target variable <span class=\"math inline\">\\(Y\\)</span>. Indeed, the term <em>generates</em> feels a lot like <em>causes</em> here. Usually, we moderate this feeling by remembering that linear regression models only correlation, and we could just have easily regressed <span class=\"math inline\">\\(X\\)</span> on <span class=\"math inline\">\\(Y\\)</span>. In this course, we learn how to formalize that feeling.</p>\n",
    "<p>We can turn this model into a generative model by placing a marginal distribution on X.</p>\n",
    "<p><span class=\"math display\">\\[\\begin{align}\n",
    "\\epsilon &amp;\\sim \\mathcal{N}(0,1)\\nonumber\\\\\n",
    "X &amp;\\sim P_X\\nonumber \\\\\n",
    "Y &amp;= \\beta X + \\alpha + \\epsilon \\nonumber\n",
    "\\end{align}\\]</span></p>\n",
    "<p>At this point, we are already telling a data generating story where <span class=\"math inline\">\\(Y\\)</span> comes from <span class=\"math inline\">\\(X\\)</span> and <span class=\"math inline\">\\(\\epsilon\\)</span>. Now let’s expand on that story. Suppose we observe that <span class=\"math inline\">\\(Y\\)</span> is measured from some instrument, and we suppose that this instrument is adding technical noise to <span class=\"math inline\">\\(Y\\)</span>. Now the regression model becomes a noise model.</p>\n",
    "<p><span class=\"math display\">\\[\\begin{align}\n",
    "\\epsilon &amp;\\sim \\mathcal{N}(0,1)\\nonumber\\\\\n",
    "X &amp;\\sim P_X\\nonumber \\\\\n",
    "Z &amp;\\sim P_Z \\nonumber\\\\\n",
    "Y &amp;= \\beta X + \\alpha + \\epsilon + Z \\nonumber\n",
    "\\end{align}\\]</span></p>\n",
    "</div>\n",
    "<div id=\"binary-classifier\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">2.3.2</span> Binary classifier</h3>\n",
    "<p>The logistic regression model has the form:</p>\n",
    "<p><span class=\"math display\">\\[\\mathbb{E}[Y] = \\texttt{logit}(\\beta X + \\alpha)\\]</span></p>\n",
    "<p>If we read this formula, it reads as Y comes from X. Of course that is not true, this model doesn’t care whether Y comes from X or vice versa, in other words, it doesn’t care how Y is generated, it merely wants to model <span class=\"math inline\">\\(P(Y=1|X)\\)</span></p>\n",
    "<p>In contrast, a naive Bayes classifier models P(X, Y) as P(X|Y)P(Y). P(X|Y) and P(Y) are estimated from the data, and then we use Bayes rule to find P(Y|X) and predict Y given X. P(X|Y)P(Y) is a representation of the data generating process that reads as “there is some unobserved Y, and then we observe X which came from Y.” There is nothing that forces us to apply naive Bayes only in problems where the generation of the prediction target generation of the features. Yet, this is precisely the kind of problem where this approach tends to get applied, such as spam detection. I argue that it P(X|Y)P(Y) aligns with a causal intuition that X comes from Y, and we avoid the inner cringe that comes from using naive Bayes when we suspect that Y comes from X. Causal modeling gives us a language to formalize this intuition.</p>\n",
    "</div>\n",
    "<div id=\"gaussian-mixture-model\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">2.3.3</span> Gaussian mixture model</h3>\n",
    "<p>The naive Bayes classifier is an example of a latent variable model. Latent variable models come with a pre-packaged data-generating story. Another example is the Gaussian mixture model.</p>\n",
    "<div class=\"figure\">\n",
    "<img src=\"../images/gmm1.png\" alt=\"Image\" />\n",
    "<p class=\"caption\">Image</p>\n",
    "</div>\n",
    "<p>Let’s recall the intuition with simple GMM with two Gaussians. We observe the data and realize that it is coming from two Gaussians with means <span class=\"math inline\">\\(\\mu_1\\)</span>, and <span class=\"math inline\">\\(\\mu_2\\)</span>. Let <span class=\"math inline\">\\(Z_i\\)</span> be a binary variable that says <span class=\"math inline\">\\(X_i\\)</span> belongs to one of these distributions.</p>\n",
    "<p>The data generating process is: 1. Some probabilistic process generated <span class=\"math inline\">\\(\\mu\\)</span>. 2. Some Dirichlet distribution generated <span class=\"math inline\">\\(\\theta\\)</span>. 3. Then for each <span class=\"math inline\">\\(i\\)</span> in some range 1. a discrete distribution with parameter <span class=\"math inline\">\\(\\theta\\)</span> generated <span class=\"math inline\">\\(Z_i\\)</span>. 2. <span class=\"math inline\">\\(Z_i\\)</span> picks a <span class=\"math inline\">\\(\\mu\\)</span> that generates <span class=\"math inline\">\\(X_i\\)</span> from a Gaussian with mean <span class=\"math inline\">\\(\\mu\\)</span>.</p>\n",
    "<p>We can represent this data generative process in code quite easily. The following pseudocode generalizes from two to k mixture components.<br />\n",
    "``` function (alphas, sigma, scale): theta = random_dirichlet(alphas))</p>\n",
    "<pre><code> for each mixture component k: \n",
    "         mu[k] = random_normal(0, sigma))\n",
    "\n",
    " for each data_point:\n",
    "         Z[i] = random_discrete(theta))\n",
    "         X[i] = random_normal(mu[Z[i]], scale))</code></pre>\n",
    "<p>```</p>\n",
    "<p>Now inferring the mixture components given data using this code requires an inference algorithm. Model-based machine learning frameworks generally let you code up the model just as above and then provide implementations of algorithms that can provide inference on the model. In the next lecture, we will cover the basics of two frameworks for model-based machine learning that implement inference algorithms.</p>\n",
    "<p>The GMM and other latent variable models like the hidden Markov model, mixed membership models like LDA, linear factor models, provide an off-the-shelf data-generating story that is straightforward to cast into code. However, just as we turned regression into a noise model, we can adjust the model and code to create a bespoke solution to a unique problem.</p>\n",
    "</div>\n",
    "<div id=\"deep-generative-models\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">2.3.4</span> Deep generative models</h3>\n",
    "<p>Deep generative models are generative models that use deep neural network architectures. Examples include variational autoencoders and generative adversarial networks. Rather than make the data generation story explicit, their basic implementation compresses all generative process into a latent encoding. But nothing is forcing them to do so. In this course, we will see examples of deep generative models where we model the critical components of the data generating process explicitly, and let the latent encoding handle nuisance variables that we don’t care about.</p>\n",
    "</div>\n",
    "</div>\n",
    "<div id=\"dont-worry-about-being-wrong\" class=\"section level2\">\n",
    "<h2><span class=\"header-section-number\">2.4</span> Don’t worry about being wrong</h2>\n",
    "<p>Deep learning works well because they essentially infer the optimal circuit between a given input signal and output channels. In contrast, when you reason about and represent as code the data generating process, you are inferring a program given program inputs and outputs.</p>\n",
    "<p>In machine learning, the task of automatically inferring a program is called program induction, and it is much harder than inferring a circuit. Indeed, that is an ill-specified problem, because there are numerous programs we could write to generate the same data. Algorithmic information theory tells us that the task of finding the shortest program that produces an output from a given input is an NP-hard problem.</p>\n",
    "<p>So if program induction is hard for computers, it should surprise us that it is challenging for humans too. In practice, we make use of domain knowledge and other constraints. For example, an economist building a price model might incorporate in their understanding of supply and demand. A computational biologist may use extensive databases of verified relationships between genes in modeling.</p>\n",
    "<p>Finally, we can still validate the model on the data using goodness-of-fit or predictive performance statistics. We can also use standard techniques for handling uncertainty in our models, such as ensemble methods.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\">4.1</span> Recap on Motivation</h2>\n",
    "<p>Our goal is to understand causal modeling within the context of generative machine learning. We just examined one generative machine learning framework called Bayesian networks (BNs) and how we can use BNs as causal models.</p>\n",
    "<p><strong>Bayesian Networks (BNs)</strong> Framework that defines a probabilistic generative model of the world in terms of a directed acyclic graph.</p>\n",
    "<p><strong>causal Bayesian networks:</strong> Bayesian networks where the direction of edges in the DAG represent causality.</p>\n",
    "<p>Bayesian networks provide a general-purpose framework for representing a causal data generating story for how the world works.</p>\n",
    "<p>Now we will introduce probabilistic programming, a framework that is more expressive than Bayesian networks.</p>\n",
    "<div id=\"what-is-a-probabilistic-programming-language\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">4.1.1</span> What is a probabilistic programming language?</h3>\n",
    "<p>“A probabilistic programming language (PPL) is a programming language designed to describe probabilistic models and then perform inference in those models. PPLs are closely related to graphical models and Bayesian networks but are more expressive and flexible. Probabilistic programming represents an attempt to”Unify general purpose programming&quot; with probabilistic modeling.&quot;</p>\n",
    "<p>-Wikipedia</p>\n",
    "<p>A PPL is a domain-specific programming language for that lets you write a data generating story as a program. As with a causal Bayesian network, you can write your program in a way that orders the steps of its execution according to cause and effect.</p>\n",
    "</div>\n",
    "<div id=\"how-exactly-do-bayesian-networks-and-probabilistic-programming-differ\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">4.1.2</span> How exactly do Bayesian networks and probabilistic programming differ?</h3>\n",
    "<p><strong>Representation of relationships between variables</strong>. BNs restricted to representing the relationships between variables in terms of conditional probability distributions (CPDs) factored according to a DAG. Frameworks typically limit you to a small set of parametric CPDs (e.g., Gaussian, multinomial).</p>\n",
    "<p>Just as computer programs are more expressive than flow charts, PPLs let you represent relations any way you like so long as you can represent them in code. PPL relationships can include control flow and recursion. In causal models, we will see that this allows you to be more specific about mechanism than you can with CPDs.</p>\n",
    "<p><strong>DAG vs. open world models</strong>. BNs restrict the representation of the joint distribution to a DAG. This constraint enables you to reason easily about the joint distribution through graph-theoretic operations like d-separation. PPLs need not be constrained to a DAG. For example (using an imaginary Python PPL package):</p>\n",
    "<pre><code>X = Bernoulli(p)\n",
    "if X == 1:\n",
    "    Y = Gaussian(0, 1)</code></pre>\n",
    "<p>In a DAG, you have a fixed set of variables, i.e. a “closed world”. In the above model, the variable Y is only instantiated if X==1. Y may or may not exist depending on how the generative process unfolds. For a more extreme example, consider this:</p>\n",
    "<pre><code>X = Poisson(λ)\n",
    "Y = [Gaussian(0, 1)]\n",
    "for i in range(1, X):\n",
    "    Y[i] = Gaussian(Y[i-1], 1))</code></pre>\n",
    "<p>Here you have the total number of Y variables itself being a random variable X. Further, the mean of the ith Y is a random variable given by the i-1th Y. You can’t do that with a Bayes net! Unfortunately, we can’t reason about this as directly as we can with a DAG. For example, recall that with the DAG, we had a convenient algorithm called <code>CPDAG</code> that converts the DAG to a partially directed acyclic graph structure called a PDAG that provides a compact representation of all the DAGs in an equivalence class. How might we define an equivalence class on this program? Certainly, enumerating all programs with an equivalent representation of the joint distribution would be very difficult even with constraints on the length of the program. In general, enumerating all programs of minimal description that provide equivalent representations of a joint distribution is an NP-hard problem.</p>\n",
    "<p><strong>Inference</strong> When you have a DAG and a constrained set of parametric CPDs, as well as constraints on the kind of inference, queries the user can make, you can implement some inference algorithms in your BN framework that will generally work in a reasonable amount of time.</p>\n",
    "<p>PPLs are more flexible than BNs, but the trade-off s that getting inference to work is harder. PPL’s develop several abstractions for inference and leave it to the user to apply them, requiring the user to become something of an expert in inference algorithms. PPL developers make design decisions to make inference easier for the user, though this often sacrifices some flexibility. One emergent pattern is to build PPLs on tensor-based frameworks like Tensorflow and PyTorch. Tensor-based PPLs allow a data scientist with experience building deep learning models to rely on that experience when doing inference.</p>\n",
    "<div class=\"figure\">\n",
    "<img src=\"../images/inference1.png\" alt=\"Image\" />\n",
    "<p class=\"caption\">Image</p>\n",
    "</div>\n",
    "<p><a href=\"https://www.youtube.com/watch?v=9SEIYh5BCjc\"><span class=\"math display\">\\[\\texttt{Kevin Smith - Tutorial: Probabilistic Programming}\\]</span></a></p>\n",
    "</div>\n",
    "</div>\n",
    "<div id=\"introduction-to-pyro\" class=\"section level2\">\n",
    "<h2><span class=\"header-section-number\">4.2</span> Introduction to Pyro</h2>\n",
    "<p>Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. Pyro enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling.</p>\n",
    "<p>Our purpose of this class, pyro has “do”-operator that allows intervention and counterfactual inference in these probabilistic models.</p>\n",
    "<div id=\"stochastic-functions\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">4.2.1</span> Stochastic Functions</h3>\n",
    "<p>The basic unit of probabilistic programs is the stochastic function. A stochastic function is an arbitrary Python callable that combines two ingredients:</p>\n",
    "<ul>\n",
    "<li>deterministic Python code; and</li>\n",
    "<li>primitive stochastic functions that call a random number generator</li>\n",
    "</ul>\n",
    "<p>For this course, we will consider these stochastic functions as <strong>models</strong>. Stochastic functions can be used to represent simplified or abstract descriptions of a data-generating process.</p>\n",
    "</div>\n",
    "<div id=\"primitive-stochastic-functions\" class=\"section level3\">\n",
    "<h3><span class=\"header-section-number\">4.2.2</span> Primitive stochastic functions</h3>\n",
    "<p>We call them distributions. We can explicitly compute the probability of the outputs given the inputs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:  tensor(-1.3905)\n"
     ]
    }
   ],
   "source": [
    "loc = 0.   # mean zero\n",
    "scale = 1. # unit variance\n",
    "normal = torch.distributions.Normal(loc, scale) # create a normal distribution object\n",
    "x = normal.rsample() # draw a sample from N(0,1)\n",
    "print(\"sample: \", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyro simplifies this process of sampling from distributions. It uses pyro.sample()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8152)\n"
     ]
    }
   ],
   "source": [
    "x = pyro.sample(\"my_sample\", pyro.distributions.Normal(loc, scale))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Just like a direct call to <code>torch.distributions.Normal().rsample()</code>, this returns a sample from the unit normal distribution. The crucial difference is that this sample is named. Pyro’s backend uses these names to uniquely identify sample statements and change their behavior at runtime depending on how the enclosing stochastic function is being used. This is how Pyro can implement the various manipulations that underlie inference algorithms.</p>\n",
    "<p>Let’s write a simple <code>weather</code> model.</p>\n",
    "\n",
    "<h3><span class=\"header-section-number\">4.2.3</span> A simple model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cloudy', 51.373016357421875)\n",
      "('cloudy', 52.28388595581055)\n",
      "('sunny', 70.28937530517578)\n"
     ]
    }
   ],
   "source": [
    "def weather():\n",
    "    cloudy = pyro.sample('cloudy', dist.Bernoulli(0.3))\n",
    "    cloudy = 'cloudy' if cloudy.item() == 1.0 else 'sunny'\n",
    "    mean_temp = {'cloudy': 55.0, 'sunny': 75.0}[cloudy]\n",
    "    scale_temp = {'cloudy': 10.0, 'sunny': 15.0}[cloudy]\n",
    "    temp = pyro.sample('temp', dist.Normal(mean_temp, scale_temp))\n",
    "    return cloudy, temp.item()\n",
    "for _ in range(3):\n",
    "    print(weather())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>First two lines introduce a binary variable <code>cloudy</code>, which is given by a draw from the Bernoulli distribution with a parameter of <span class=\"math inline\">\\(0.3\\)</span>. The Bernoulli distribution returns either <span class=\"math inline\">\\(0\\)</span> or <span class=\"math inline\">\\(1\\)</span>, line <code>2</code> converts that into a string. So, So according to this model, <span class=\"math inline\">\\(30%\\)</span> of the time it’s cloudy and <span class=\"math inline\">\\(70%\\)</span> of the time it’s sunny.</p>\n",
    "<p>In line <code>4</code> and <code>5</code>, we initialize mean and scale of the temperature for both values. We then sample, the temperature from a Normal distribution and return that along with <code>cloudy</code> variable.</p>\n",
    "<p>We can build complex model by modularizing and reusing the concepts into functions and use them as programmers use functions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ice_cream_sales():\n",
    "    cloudy, temp = weather()\n",
    "    expected_sales = 200. if cloudy == 'sunny' and temp > 80.0 else 50.\n",
    "    ice_cream = pyro.sample('ice_cream', pyro.distributions.Normal(expected_sales, 10.0))\n",
    "    return ice_cream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><span class=\"header-section-number\">4.3</span> Inference</h2>\n",
    "<p>As we discussed earlier, the reason we use PPLs is because they can easily go backwards and reason about cause given the observed effect. There are myriad of inference algorithms available in pyro. Let’s try it on an even simpler model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9213)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scale(guess):\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(guess, 1.0))\n",
    "    measurement = pyro.sample(\"measurement\", dist.Normal(weight, 0.75))\n",
    "    return measurement\n",
    "scale(5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Suppose we observe that the <code>measurement</code> of an object was <span class=\"math inline\">\\(14\\)</span> lbs. What would have we guessed if we tried to guess it’s <code>weight</code> first?</p>\n",
    "<p>This question is answered in two steps.</p>\n",
    "<ol style=\"list-style-type: decimal\">\n",
    "<li>Condition the model.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioned_scale = pyro.condition(scale, data={\"measurement\": torch.tensor(14.)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start=\"2\" style=\"list-style-type: decimal\">\n",
    "<li>Set the prior and infer the posterior. We will use</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sample: 100%|██████████| 1050/1050 [00:04<00:00, 262.34it/s, step size=1.31e+00, acc. rate=0.930]\n"
     ]
    }
   ],
   "source": [
    "from pyro.infer.mcmc import MCMC\n",
    "from pyro.infer.mcmc.nuts import HMC\n",
    "from pyro.infer import EmpiricalMarginal\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "guess_prior = 10.\n",
    "hmc_kernel = HMC(conditioned_scale, step_size=0.9, num_steps=4)\n",
    "posterior = MCMC(hmc_kernel, num_samples=1000, warmup_steps=50).run(guess_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, '#')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "marginal = EmpiricalMarginal(posterior, \"weight\")\n",
    "plt.hist([marginal().item() for _ in range(1000)],)\n",
    "plt.title(\"P(weight | measurement = 14)\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><span class=\"header-section-number\">4.3.0.1</span> Shapes in distribution:</h4>\n",
    "<p>We know that PyTorch tensor have single <code>shape</code> attribute, <code>Distribution</code>s have two shape attributes with special meaning. * <code>.batch_shape</code>: Indices over <code>.batch_shape</code> denote conditionally independent random variables, * <code>.event_shape</code>: indices over <code>.event_shape</code> denote dependent random variables (ie one draw from a distribution).</p>\n",
    "<p>These two combine to define the total shape of a sample. Thus the total shape of <code>.log_prob()</code> of distribution is <code>.batch_shape</code>.</p>\n",
    "<p>Also, <code>Distribution.sample()</code> also has a <code>sample_shape</code> attribute that indexes over independent and identically distributed(iid) random variables.</p>\n",
    "\n",
    "\n",
    "<pre><code>      |      iid     | independent | dependent\n",
    "------+--------------+-------------+------------\n",
    "shape = sample_shape + batch_shape + event_shape</code></pre>\n",
    "<p>To know more about + , go through <a href=\"https://pytorch.org/docs/master/notes/broadcasting.html\">broadcasting tensors in PyTorch</a>.</p>\n",
    "\n",
    "<h3><span class=\"header-section-number\">4.3.1</span> Examples</h3>\n",
    "<p>One way to introduce batch_shape is use <code>expand</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_shape:  torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "d = dist.MultivariateNormal(torch.zeros(3), torch.eye(3, 3)).expand([5]) # expand - 3 of these Multivariate Normal Dists\n",
    "print(\"batch_shape: \", d.batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_shape:  torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(\"event_shape: \", d.event_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape:  torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = d.sample()\n",
    "print(\"x shape: \", x.shape)          # == sample_shape + batch_shape + event_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d.log_prob(x) shape: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "print(\"d.log_prob(x) shape:\", d.log_prob(x).shape)  # == batch_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The other way is using <code>plate</code> context manager.</p>\n",
    "<p>Pyro models can use the context manager <code>pyro.plate</code> to declare that certain batch dimensions are independent. Inference algorithms can then take advantage of this independence to e.g. construct lower variance gradient estimators or to enumerate in linear space rather than exponential space.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pyro.plate(\"x_axis\", 5):\n",
    "    d = dist.MultivariateNormal(torch.zeros(3), torch.eye(3, 3))\n",
    "    x = pyro.sample(\"x\", d)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In fact, we can also nest <code>plates</code>. The only thing we need to care about is, which dimensions are independent. Pyro automatically manages this but sometimes we need to explicitely specify the dimensions. Once we specify that, we can leverage PyTorch’s CUDA enabled capabilities to run inference on GPUs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-17-c756cd874096>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-c756cd874096>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    # within this context, batch dimensions -2 and -1 are independent\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "with pyro.plate(\"x_axis\", 320):\n",
    "    # within this context, batch dimension -1 is independent\n",
    "    with pyro.plate(\"y_axis\", 200):\n",
    "        # within this context, batch dimensions -2 and -1 are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Note that we always count from the right by using negative indices like <span class=\"math inline\">\\(-2\\)</span>, <span class=\"math inline\">\\(-1\\)</span>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
