{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f59fcd9eb70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.infer as infer\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import math\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro.distributions as dist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "torch.manual_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of stochastic variational inference\n",
    "\n",
    "\"\"\"\n",
    "Introduce a variational distribution with parameters called as variational parameters\n",
    "In the context of pyro, this distribution is known as the guide\n",
    "\n",
    "The guide serves as an approximation to the true posterior\n",
    "\n",
    "\n",
    "To move the guide closer to the posterior, we need an appropriate objective function.\n",
    "\n",
    "We use the Evidence Lower Bound (ELBO)\n",
    "\n",
    "ELBO‚â°ùîºqœï(z)[logpŒ∏(x,z)‚àílogqœï(z)]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def model(data):\n",
    "    \n",
    "    # define the parameters that control the beta prior\n",
    "    alpha_0 = torch.Tensor([10.0])\n",
    "    beta_0 = torch.Tensor([10.0])\n",
    "    \n",
    "    # sample fairness f from beta prior\n",
    "    f = pyro.sample('latent_fairness', dist.Beta(alpha_0, beta_0))\n",
    "    # Loop over the observed data\n",
    "    for i in range(len(data)):\n",
    "        # observe datapoint i using the bernoulli\n",
    "        # likelihood Bernoulli(f)\n",
    "        pyro.sample(\"obs_{}\".format(i), dist.Bernoulli(f), obs=data[i])\n",
    "        # Conditioned on the 'latent fairness' random variable, we observe each of the datapoints using a bernoulli likelihood\n",
    "\n",
    "# Define a corresponding guide i.e. an appropriate variational\n",
    "# distribution for the latent random variable f.\n",
    "\n",
    "\n",
    "def guide(data):\n",
    "    # register the 2 variational parameters with pyro\n",
    "    alpha_q = pyro.param('alpha_q', torch.Tensor([15.0]), constraint=constraints.positive)\n",
    "    beta_q = pyro.param('beta_q', torch.Tensor([15.0]), constraint=constraints.positive)\n",
    "\n",
    "    # sample latent_fairness from the distribution Beta(alpha_q, beta_q)\n",
    "    pyro.sample(\"latent_fairness\", dist.Beta(alpha_q, beta_q))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable validation (e.g. validate parameters of distributions)\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "# clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# create some data with 6 observed heads and 4 observed tails\n",
    "data = []\n",
    "for _ in range(6):\n",
    "    data.append(torch.Tensor([1.0]))\n",
    "for _ in range(4):\n",
    "    data.append(torch.Tensor([0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................."
     ]
    }
   ],
   "source": [
    "# setup the optimizer\n",
    "adam_params = {\"lr\": 0.0005, \"betas\": (0.90, 0.999)}\n",
    "optimizer = Adam(adam_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "\n",
    "# do gradient steps\n",
    "for step in range(2000):\n",
    "    svi.step(data)\n",
    "    if step % 100 == 0:\n",
    "        print('.', end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the learned variational parameters\n",
    "alpha_q = pyro.param(\"alpha_q\").item()\n",
    "beta_q = pyro.param(\"beta_q\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "based on the data and our prior belief, the fairness of the coin is 0.540 +- 0.090\n"
     ]
    }
   ],
   "source": [
    "# here we use some facts about the beta distribution\n",
    "# compute the inferred mean of the coin's fairness\n",
    "inferred_mean = alpha_q / (alpha_q + beta_q)\n",
    "# compute inferred standard deviation\n",
    "factor = beta_q / (alpha_q * (1.0 + alpha_q + beta_q))\n",
    "inferred_std = inferred_mean * math.sqrt(factor)\n",
    "\n",
    "print(\"\\nbased on the data and our prior belief, the fairness \" +\n",
    "      \"of the coin is %.3f +- %.3f\" % (inferred_mean, inferred_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
